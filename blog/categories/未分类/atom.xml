<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: 未分类 | 调和的微光-HARMONIC GLEAM]]></title>
  <link href="http://hmgle.github.com/blog/categories/未分类/atom.xml" rel="self"/>
  <link href="http://hmgle.github.com/"/>
  <updated>2013-02-21T22:42:51+08:00</updated>
  <id>http://hmgle.github.com/</id>
  <author>
    <name><![CDATA[hmgle]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[嵌入式Linux基于framebuffer的jpeg格式本地LCD屏显示]]></title>
    <link href="http://hmgle.github.com/blog/2011/10/18/e5b58ce585a5e5bc8flinuxe59fbae4ba8eframebuffere79a84jpege6a0bce5bc8fe69cace59cb0lcde5b18fe698bee7a4ba/"/>
    <updated>2011-10-18T07:20:31+08:00</updated>
    <id>http://hmgle.github.com/blog/2011/10/18/e5b58ce585a5e5bc8flinuxe59fbae4ba8eframebuffere79a84jpege6a0bce5bc8fe69cace59cb0lcde5b18fe698bee7a4ba</id>
    <content type="html"><![CDATA[<p>在基于Linux的视频监控采集系统中,摄像头采集到的一帧视频图像数据一般都是经过硬件自动压缩成jpeg格式的,然后再保存到摄像头设备的缓冲区.如果要把采集到的jpeg格式显示在本地LCD屏上,由于我们的Linux系统没有移植任何GUI系统,就要考虑以下方面:
1. 将jpeg格式解压缩为位图格式,也就是jpeg解码.</p>

<ol>
<li><p>将解码出来的位图格式输出到本地的LCD屏上. 在Linux系统下是通过写入帧缓冲(framebuffer)来实现的.</p></li>
<li><p>framebuffer相当于为LCD设备提供一个统一的接口,对framebuffer的操控会反映到LCD显示设备上去. 如果配置Linux内核时没有找到支持本地lcd屏这种型号的驱动,那我们要自己写lcd屏驱动,然后选择静态加入内核或以模块的形式加入内核动态加载.</p></li>
</ol>


<p>针对以上三点,我们逐一解决:</p>

<h2>1. jpeg解码</h2>

<p>先了解一下jpeg标准的编码过程:原始的一帧未压缩过的图像可以看成是RGB(红绿蓝)色彩空间上的一组向量集合,但在RGB空间是不利于数据压缩的,因此为了压缩先要把图像映射到利于压缩的YUV空间上(原因是因为人类的眼睛对于亮度差异的敏感度高于色彩变化,而YUV空间的一个基向量Y就是亮度), 这一步叫色彩空间转换.下一步可以在YUV空间上减少U(色调)和V(饱和度)的成分,也就是在亮度信息不减少的情况下移除部分色彩信息,谁叫人的眼睛对亮度的敏感优于对色彩的敏感呢.这一步叫缩减取样.下一步是将图像从色彩空间映射到频率空间,可以采用的变换方法有:离散余弦变换, 傅氏变换, 正弦变换等. 其中应用最广的是离散余弦变换(DCT).这一步是无损的,目的是为了在下一步称之为量化的过程中可以经过四舍五入删除高频量得到压缩后的矩阵.量化之后就是对这个矩阵的编码问题了.针对这个矩阵的分布特点, 采用"Z"字形的顺序扫描编排,然后进行RLE行程编码, 把大量连续重复的数据压缩.最后再用范式Huffman编码.要了解详细的过程,可以查看<a href="http://www.jpeg.org/">JPEG标准</a>.</p>

<p>而解码就是以上编码的逆过程了.除非想要自己实现jpeg的编码和解码函数,我们可以不必细究这些过程,而是直接使用别人已经实现的jpeg编码解码库.在Linux平台下, 有libjpeg库, 它是完全用C语言编写的, 依照它的许可协议,可自由使用, 不是GPL协议,它可以用于商业目的.</p>

<p>libjpeg的6b版本有个问题,就是解码接口,它只接受文件源.打开源的函数<code>jpeg_stdio_src(j_decompress_ptr cinfo, FILE *infile)</code>要求解码源infile是文件.而我们希望解码的是直接来自映射内存中的数据.要解码内存流的话就要修改libjpeg的源码了,可以参考这里:<a href="http://my.unix-center.net/~Simon_fu/?p=565">http://my.unix-center.net/~Simon_fu/?p=565</a> 目前libjpeg的最新版8c已经解决了这个接口不好的问题了,它增加了对内存流解码的支持.通过调用函数</p>

<pre><code>jpeg_mem_src(&amp;cinfo, fdmem, st.st_size);
</code></pre>

<p>就可以将保存在内存的jpeg格式数据作为源输入了.因此我们就用libjpeg 8c这个版本来解码.</p>

<p>用到的函数主要有:</p>

<ol>
<li><p>初始化jpeg解压对象:</p>

<p> /<em> init jpeg decompress object error handler </em>/
 cinfo.err = jpeg_std_error(&amp;jerr);
 jpeg_create_decompress(&amp;cinfo);</p></li>
<li><p>绑定jpeg解压对象到输入流:</p>

<p>   /<em> bind jpeg decompress object to infile </em>/</p>

<h1>if READ_FILE   // 从jpeg文件读入</h1>

   jpeg_stdio_src(&amp;cinfo, infile);

<h1>elif READ_MEM  // 从内存读入jpeg格式</h1>

   jpeg_mem_src(&amp;cinfo, fdmem, st.st_size);

<h1>endif</h1></li>
<li><p>读取jpeg头部信息:</p>

<p>   /<em> read jpeg header </em>/
   jpeg_read_header(&amp;cinfo, TRUE);</p></li>
<li><p>解压过程:</p>

<p>   /<em> decompress process </em>/
   jpeg_start_decompress(&amp;cinfo);</p></li>
</ol>


<p>调用这个函数之后,可以得到jpeg图像的下面几个参数:</p>

<ol>
<li><p>output_width // 图像的宽度</p></li>
<li><p>output_height // 图像的高度</p></li>
<li><p>output_components // 每个像素占用的字节数</p></li>
</ol>


<p>我们采用每扫描一行像素就输出到屏幕的方法的话,根据以上参数可以确定分配一行信息需要的缓冲区:</p>

<pre><code>    buffer = (unsigned char *)malloc(cinfo.output_width *
            cinfo.output_components);
</code></pre>

<p>总共需要扫描output_height行.</p>

<ol>
<li><p>读取一行扫描数据并输出到LCD屏幕:</p>

<p>   y = 0;
   while (cinfo.output_scanline &lt; cinfo.output_height) {</p>

<pre><code>   jpeg_read_scanlines(&amp;cinfo, &amp;buffer, 1);
   if (fb_depth == 16) {   // 如果显示设备色深是16位
       ...
   } else if (fb_depth == 24) {    // 如果显示设备色深是24位
       ...
   } else if (fb_depth == 32) {    // 如果显示设备色深是32位
       ...
   }
   y++;
</code></pre>

<p>   }</p></li>
<li><p>结束jpeg解码:</p>

<p>   /<em> finish decompress, destroy decompress object </em>/
   jpeg_finish_decompress(&amp;cinfo);
   jpeg_destroy_decompress(&amp;cinfo);</p></li>
<li><p>释放缓冲区:</p>

<p>   /<em> release memory buffer </em>/
   free(buffer);</p></li>
</ol>


<h2>2. 输出位图到LCD屏</h2>

<p>通过framebuffer直接写屏的主要步骤有:</p>

<ol>
<li><p>打开framebuffer设备:</p>

<p>   /<em> open framebuffer device </em>/
   fbdev = fb_open("/dev/fb0");</p></li>
<li><p>获取framebuffer设备参数:</p>

<p>   /<em> get status of framebuffer device </em>/
   fb_stat(fbdev, &amp;fb_width, &amp;fb_height, &amp;fb_depth);</p></li>
<li><p>映射framebuffer设备到共享内存:</p>

<p>   screensize = fb_width * fb_height * fb_depth / 8;
   fbmem = fb_mmap(fbdev, screensize);</p></li>
<li><p>直接对映射到那片内存进行写操作,LCD屏刷新刷新时就会反应到屏幕上去了.</p>

<p>   y = 0;
   while (cinfo.output_scanline &lt; cinfo.output_height) {</p>

<pre><code>   jpeg_read_scanlines(&amp;cinfo, &amp;buffer, 1);
   if (fb_depth == 16) {
       unsigned short color;

       for (x = 0; x &lt; cinfo.output_width; x++) {
           color =
               RGB888toRGB565(buffer[x * 3],
                       buffer[x * 3 + 1], buffer[x * 3 + 2]);
           fb_pixel(fbmem, fb_width, fb_height, x, y, color);
       }
   } else if (fb_depth == 24) {
       memcpy((unsigned char *) fbmem + y * fb_width * 3,
               buffer, cinfo.output_width * cinfo.output_components);
   } else if (fb_depth == 32) {
       // memcpy((unsigned char *) fbmem + y * fb_width * 4,
               // buffer, cinfo.output_width * cinfo.output_components);
       for (x = 0; x &lt; cinfo.output_width; x++) {
           *(fbmem + y * fb_width * 4 + x * 4)     = (unsigned char) buffer[x * 3 + 2];
           *(fbmem + y * fb_width * 4 + x * 4 + 1) = (unsigned char) buffer[x * 3 + 1];
           *(fbmem + y * fb_width * 4 + x * 4 + 2) = (unsigned char) buffer[x * 3 + 0];
           *(fbmem + y * fb_width * 4 + x * 4 + 3) = (unsigned char) 0;
       }
   }
   y++;    // next scanline
</code></pre>

<p>   }</p></li>
<li><p>卸载映射framebuffer的那部分内存:</p>

<p>   /<em> unmap framebuffer's shared memory </em>/
   fb_munmap(fbmem, screensize);</p></li>
<li><p>关闭framebuffer设备:</p>

<p>   close(fbdev);</p></li>
</ol>


<p>根据以上两点,可以写一个测试程序,在不开X-window图形系统的情况下,将本地的jpeg文件直接显示到屏幕上.</p>

<pre><code>#include    &lt;stdio.h&gt;
#include    &lt;string.h&gt;
#include    &lt;stdlib.h&gt;
#include    &lt;unistd.h&gt;
#include    &lt;sys/ioctl.h&gt;
#include    &lt;sys/types.h&gt;
#include    &lt;sys/stat.h&gt;
#include    &lt;errno.h&gt;
#include    &lt;fcntl.h&gt;
#include    &lt;sys/mman.h&gt;
#include    &lt;linux/fb.h&gt;
#include    "jpeglib.h"
#include    "jerror.h"

#define FB_DEV  "/dev/fb0"
#define __fnc__ __FUNCTION__

#define debug           0
#define debug_printf    0
#define BYREAD          0
#define BYMEM           1

/* function deciaration */

void usage(char *msg);
unsigned short RGB888toRGB565(unsigned char red,
        unsigned char green, unsigned char blue);
int fb_open(char *fb_device);
int fb_close(int fd);
int fb_stat(int fd, unsigned int *width, unsigned int *height, unsigned int *    depth);
void *fb_mmap(int fd, unsigned int screensize);
void *fd_mmap(int fd, unsigned int filesize);
int fb_munmap(void *start, size_t length);
int fb_pixel(void *fbmem, int width, int height,
        int x, int y, unsigned short color);

#if(debug)
void draw(unsigned char *fbp,
        struct fb_var_screeninfo vinfo,
        struct fb_fix_screeninfo finfo);
#endif

/* function implementation */

int main(int argc, char **argv)
{
    struct jpeg_decompress_struct cinfo;
    struct jpeg_error_mgr jerr;
#if(BYREAD)
    FILE *infile;
#endif
    int fd;
    unsigned char *buffer;
    struct stat st;

    int fbdev;
    char *fb_device;
    unsigned char *fbmem;
    unsigned char *fdmem;
    unsigned int screensize;
    unsigned int fb_width;
    unsigned int fb_height;
    unsigned int fb_depth;
    register unsigned int x;
    register unsigned int y;

    /* check auguments */
    if (argc != 2) {
        usage("insuffient auguments");
        exit(-1);
    }

    /* open framebuffer device */
    if ((fb_device = getenv("FRAMEBUFFER")) == NULL)
        fb_device = FB_DEV;
    fbdev = fb_open(fb_device);

    /* get status of framebuffer device */
    fb_stat(fbdev, &amp;fb_width, &amp;fb_height, &amp;fb_depth);

    /* map framebuffer device to shared memory */
    screensize = fb_width * fb_height * fb_depth / 8;
    fbmem = fb_mmap(fbdev, screensize);

#if (BYREAD)
    /* open input jpeg file */
    if ((infile = fopen(argv[1], "rb")) == NULL) {
        fprintf(stderr, "open %s failed\n", argv[1]);
        exit(-1);
    }
#endif

    if ((fd = open(argv[1], O_RDONLY)) &lt; 0) {
        perror("open");
        exit(-1);
    }

    if (fstat(fd, &amp;st) &lt; 0) {
        perror("fstat");
        exit(-1);
    }

    fdmem = fd_mmap(fd, st.st_size);

    /* init jpeg decompress object error handler */
    cinfo.err = jpeg_std_error(&amp;jerr);
    jpeg_create_decompress(&amp;cinfo);

    /* bind jpeg decompress object to infile */
#if (BYREAD)
    jpeg_stdio_src(&amp;cinfo, infile);
#endif

#if (BYMEM)
    jpeg_mem_src(&amp;cinfo, fdmem, st.st_size);
#endif

    /* read jpeg header */
    jpeg_read_header(&amp;cinfo, TRUE);

    /* decompress process */
    jpeg_start_decompress(&amp;cinfo);
    if ((cinfo.output_width &gt; fb_width) ||
            (cinfo.output_height &gt; fb_height)) {
        printf("too large jpeg file, can't display\n");
#if (0)
        return -1;
#endif
    }

    buffer = (unsigned char *) malloc(cinfo.output_width *
            cinfo.output_components);

    struct fb_fix_screeninfo fb_finfo;
    struct fb_var_screeninfo fb_vinfo;

    if (ioctl(fbdev, FBIOGET_FSCREENINFO, &amp;fb_finfo)) {
        perror(__fnc__);
        return -1;
    }

    if (ioctl(fbdev, FBIOGET_VSCREENINFO, &amp;fb_vinfo)) {
        perror(__fnc__);
        return -1;
    }

#if(debug)
    draw(fbmem, fb_vinfo, fb_finfo);
#endif
    y = 0;
    while (cinfo.output_scanline &lt; cinfo.output_height) {
        jpeg_read_scanlines(&amp;cinfo, &amp;buffer, 1);
        if (fb_depth == 16) {
            unsigned short color;

            for (x = 0; x &lt; cinfo.output_width; x++) {
                color =
                    RGB888toRGB565(buffer[x * 3],
                            buffer[x * 3 + 1], buffer[x * 3 + 2]);
                fb_pixel(fbmem, fb_width, fb_height, x, y, color);
            }
        } else if (fb_depth == 24) {
            memcpy((unsigned char *) fbmem + y * fb_width * 3,
                    buffer, cinfo.output_width * cinfo.output_components);
        } else if (fb_depth == 32) {
            // memcpy((unsigned char *) fbmem + y * fb_width * 4,
                    // buffer, cinfo.output_width * cinfo.output_components);
            for (x = 0; x &lt; cinfo.output_width; x++) {
                * (fbmem + y * fb_width * 4 + x * 4)     = (unsigned char)       buffer[x * 3 + 2];
                * (fbmem + y * fb_width * 4 + x * 4 + 1) = (unsigned char)       buffer[x * 3 + 1];
                * (fbmem + y * fb_width * 4 + x * 4 + 2) = (unsigned char)       buffer[x * 3 + 0];
                * (fbmem + y * fb_width * 4 + x * 4 + 3) = (unsigned char) 0;
            }
        }
        y++;    // next scanline
    }

    /* finish decompress, destroy decompress object */
    jpeg_finish_decompress(&amp;cinfo);
    jpeg_destroy_decompress(&amp;cinfo);

    /* release memory buffer */
    free(buffer);

#if (BYREAD)
    /* close jpeg inputing file */
    fclose(infile);
#endif

    /* unmap framebuffer's shared memory */
    fb_munmap(fbmem, screensize);

#if (BYMEM)
    munmap(fdmem, (size_t) st.st_size);
    close(fd);
#endif

    /* close framebuffer device */
    fb_close(fbdev);

    return 0;
}

void usage(char *msg)
{
    fprintf(stderr, "%s\n", msg);
    printf("Usage: fv some-jpeg-file.jpg\n");
}

/* open framebuffer device.
 * return positive file descriptor if success,
 * else return -1
 */
int fb_open(char *fb_device)
{
    int fd;

    if ((fd = open(fb_device, O_RDWR)) &lt; 0) {
        perror(__fnc__);
        return -1;
    }
    return fd;
}

int fb_close(int fd)
{
    return (close(fd));
}

/* get framebuffer's width, height, and depth.
 * return 0 if success, else return -1.
 */
int fb_stat(int fd, unsigned int *width, unsigned int *height, unsigned int *    depth)
{
    struct fb_fix_screeninfo fb_finfo;
    struct fb_var_screeninfo fb_vinfo;

    if (ioctl(fd, FBIOGET_FSCREENINFO, &amp;fb_finfo)) {
        perror(__fnc__);
        return -1;
    }

    if (ioctl(fd, FBIOGET_VSCREENINFO, &amp;fb_vinfo)) {
        perror(__fnc__);
        return -1;
    }

    *width = fb_vinfo.xres;
    *height = fb_vinfo.yres;
    *depth = fb_vinfo.bits_per_pixel;

    return 0;
}

/* map shared memory to framebuffer device.
 * return maped memory if success
 * else return -1, as mmap dose
 */
void *fb_mmap(int fd, unsigned int screensize)
{
    caddr_t fbmem;

    if ((fbmem = mmap(0, screensize, PROT_READ | PROT_WRITE,
                    MAP_SHARED, fd, 0)) == MAP_FAILED) {
        perror(__func__);
        return (void *) (-1);
    }

    return fbmem;
}

/* map shared memmory to a opened file */
void *fd_mmap(int fd, unsigned int filesize)
{
    caddr_t fdmem;

    if ((fdmem = mmap(0, filesize, PROT_READ,
                    MAP_SHARED, fd, 0)) == MAP_FAILED) {
        perror(__func__);
        return (void *) (-1);
    }

    return fdmem;
}

/* unmap map memory for framebuffer device */
int fb_munmap(void *start, size_t length)
{
    return (munmap(start, length));
}

/* convert 24bit RGB888 to 16bit RGB565 color format */
unsigned short RGB888toRGB565(unsigned char red,
        unsigned char green, unsigned char blue)
{
    unsigned short B = (blue &gt;&gt; 3) &amp; 0x001F;
    unsigned short G = ((green &gt;&gt; 2) &lt;&lt; 5) &amp; 0x07E0;
    unsigned short R = ((red &gt;&gt; 3) &lt;&lt; 11) &amp; 0xF800;

    return (unsigned short) (R | G | B);
}

/* display a pixel on the framebuffer device.
 * fbmem is the starting memory of framebuffer,
 * width and height are dimension of framebuffer,
 * width and height are dimension of framebuffer,
 * x and y are the coordinates to display,
 * color is the pixel's color value.
 * return 0 if success, otherwise return -1.
 */
int fb_pixel(void *fbmem, int width, int height,
        int x, int y, unsigned short color)
{
    if ((x &gt; width) || (y &gt; height))
        return -1;

    unsigned short *dst = ((unsigned short *) fbmem + y * width + x);

    *dst = color;
    return 0;
}
</code></pre>

<h2>3. LCD驱动</h2>

<p>我们用到的是一块东华3.5寸数字屏,型号为WXCAT35-TG3.下面的驱动程序是韦东山老师课堂上现场写的,如下:</p>

<pre><code>#include &lt;linux/module.h&gt;
#include &lt;linux/kernel.h&gt;
#include &lt;linux/errno.h&gt;
#include &lt;linux/string.h&gt;
#include &lt;linux/mm.h&gt;
#include &lt;linux/slab.h&gt;
#include &lt;linux/delay.h&gt;
#include &lt;linux/interrupt.h&gt;
#include &lt;linux/fb.h&gt;
#include &lt;linux/init.h&gt;
#include &lt;linux/ioport.h&gt;
#include &lt;linux/dma-mapping.h&gt;

#include &lt;asm/uaccess.h&gt;
#include &lt;asm/system.h&gt;
#include &lt;asm/irq.h&gt;
#include &lt;asm/setup.h&gt;

/* WXCAT35-TG3 */
struct s3c_lcd_regs {
    unsigned long   lcdcon1;
    unsigned long   lcdcon2;
    unsigned long   lcdcon3;
    unsigned long   lcdcon4;
    unsigned long   lcdcon5;
    unsigned long   lcdsaddr1;
    unsigned long   lcdsaddr2;
    unsigned long   lcdsaddr3;
    unsigned long   redlut;
    unsigned long   greenlut;
    unsigned long   bluelut;
    unsigned long   reserved[9];
    unsigned long   dithmode;
    unsigned long   tpal;
    unsigned long   lcdintpnd;
    unsigned long   lcdsrcpnd;
    unsigned long   lcdintmsk;
    unsigned long   lpcsel;
};

static u32 colregs[16];
static struct fb_info *s3c_fb_info;
static dma_addr_t s3c_fb_handle;
static unsigned long fb_va;

/* from pxafb.c */
static inline unsigned int chan_to_field(unsigned int chan, struct fb_bitfield *bf)
{
    chan &amp;= 0xffff;
    chan &gt;&gt;= 16 - bf-&gt;length;
    return chan &lt;&lt; bf-&gt;offset;
}

static int s3cfb_setcolreg(unsigned regno,
                   unsigned red, unsigned green, unsigned blue,
                   unsigned transp, struct fb_info *info)
{
    unsigned int val;

    /* dprintk("setcol: regno=%d, rgb=%d,%d,%d\n", regno, red, green, blue); */

    /* true-colour, use pseuo-palette */

    if (regno &lt; 16) {
        u32 *pal = s3c_fb_info-&gt;pseudo_palette;

        val  = chan_to_field(red,   &amp;s3c_fb_info-&gt;var.red);
        val |= chan_to_field(green, &amp;s3c_fb_info-&gt;var.green);
        val |= chan_to_field(blue,  &amp;s3c_fb_info-&gt;var.blue);

        pal[regno] = val;
    }

    return 0;
}

static struct fb_ops s3cfb_ops = {
    .owner      = THIS_MODULE,
//  .fb_check_var   = clps7111fb_check_var,
//  .fb_set_par = clps7111fb_set_par,
//  .fb_setcolreg   = clps7111fb_setcolreg,
//  .fb_blank   = clps7111fb_blank,

    .fb_setcolreg   = s3cfb_setcolreg,
    .fb_fillrect    = cfb_fillrect,
    .fb_copyarea    = cfb_copyarea,
    .fb_imageblit   = cfb_imageblit,
};

struct s3c_lcd_regs *s3c_lcd_regs;
static volatile unsigned long *gpccon;
static volatile unsigned long *gpdcon;
static volatile unsigned long *gpgcon;

int s3c_lcd_init(void)
{
    extern int debug_lcd;
    /* 1. 分配一个fb_info结构体 */
    s3c_fb_info = framebuffer_alloc(0, NULL);
    printk("%s %d\n", __FUNCTION__, __LINE__);

    /* 2. 设置fb_info结构体 */
    /*
       2.1 设置固定的信息
       2.2 设置可变的信息
       2.3 设置操作函数
    */

    /* 24BPP(bits per pixel), 会用到4字节, 其中浪费1字节 */
    strcpy(s3c_fb_info-&gt;fix.id, "WXCAT35-TG3");
    // s3c_fb_info-&gt;fix.smem_start // frame buffer's physical address
    s3c_fb_info-&gt;fix.smem_len    = 320*240*32/8;
    s3c_fb_info-&gt;fix.type        = FB_TYPE_PACKED_PIXELS;
    s3c_fb_info-&gt;fix.visual      = FB_VISUAL_TRUECOLOR;
    s3c_fb_info-&gt;fix.line_length = 320 * 4;

    s3c_fb_info-&gt;var.xres             = 320;
    s3c_fb_info-&gt;var.yres             = 240;
    s3c_fb_info-&gt;var.xres_virtual     = 320;
    s3c_fb_info-&gt;var.yres_virtual     = 240;
    s3c_fb_info-&gt;var.bits_per_pixel   = 32;

    s3c_fb_info-&gt;var.red.offset       = 16;
    s3c_fb_info-&gt;var.red.length       = 8;

    s3c_fb_info-&gt;var.green.offset     = 8;
    s3c_fb_info-&gt;var.green.length     = 8;

    s3c_fb_info-&gt;var.blue.offset      = 0;
    s3c_fb_info-&gt;var.blue.length      = 8;

    //s3c_fb_info-&gt;var.activate         = FB_ACTIVATE;

    s3c_fb_info-&gt;fbops                = &amp;s3cfb_ops;
    s3c_fb_info-&gt;pseudo_palette       = colregs;

    /* 3. 硬件相关的操作 */
    /* 配置GPIO */
    gpccon     = ioremap(0x56000020, 4);
    gpdcon     = ioremap(0x56000030, 4);
    gpgcon     = ioremap(0x56000060, 4);
    *gpccon = 0xaaaaaaaa;
    *gpdcon = 0xaaaaaaaa;
    *gpgcon |= (3&lt;&lt;8);  /* GPG4 use as lcd_pwren */
    printk("%s %d\n", __FUNCTION__, __LINE__);

    s3c_lcd_regs = ioremap(0X4D000000, sizeof(struct s3c_lcd_regs));

    /*
     * VCLK = HCLK / [(CLKVAL+1)x2] = 100M/[(CLKVAL+1)x2] = 6.4
     * CLKVAL = 6.8 = 7
     * TFT LCD panel
     * 24bpp
     */
    s3c_lcd_regs-&gt;lcdcon1 = (7&lt;&lt;8)|(0&lt;&lt;7)|(3&lt;&lt;5)|(0x0d&lt;&lt;1)|(0&lt;&lt;0);
    printk("%s %d\n", __FUNCTION__, __LINE__);

    /* VBPD: 电子枪收到VSYNC信号后,"多长时间"才能跳回第1行
     * VBPD=14,      LCD: tvb=15
     * LINEVAL=239,  LCD: 有240行
     * VFPD=11,      LCD: tvf=12  // 发出最后一行数据后,再过多长时间才发出VSYNC
     * VSPW=2,       LCD: tvp=3   // VSYNC的宽度
     */
    s3c_lcd_regs-&gt;lcdcon2 = (14&lt;&lt;24)|(239&lt;&lt;14)|(11&lt;&lt;6)|(2&lt;&lt;0);

    /* HBPD: 电子枪收到HSYNC信号后,"多长时间"才能跳回第1列
     * HBPD=37,      LCD: thb=38
     * HORVAL=319,   LCD: 有320行
     * HFPD=19,      LCD: thf=20  // 发出最后一象素数据后,再过多长时间才发出HSYNC
     * HSPW=29,      LCD: thp=30   // VSYNC的宽度
     */
    s3c_lcd_regs-&gt;lcdcon3 = (37&lt;&lt;19)|(319&lt;&lt;8)|(19&lt;&lt;0);
    s3c_lcd_regs-&gt;lcdcon4 = 29;

    /* bit10:  在VCLK上升沿取数据 
     * bit9 :  VSYNC低电平有效
     * bit8 :  HSYNC低电平有效
     * bit5 :  PWREN低电平有效
     */ 
    s3c_lcd_regs-&gt;lcdcon5 = (1&lt;&lt;10)|(1&lt;&lt;9)|(1&lt;&lt;8)|(1&lt;&lt;5)|(0&lt;&lt;3);

    /* 分配frame buffer */
    fb_va = (unsigned long)dma_alloc_writecombine(NULL, s3c_fb_info-&gt;fix.smem_len, &amp;s3c_fb_handle, GFP_KERNEL);

    printk("fb_va = 0x%x, pa = 0x%x\n", fb_va, s3c_fb_handle);
    s3c_fb_info-&gt;fix.smem_start = s3c_fb_handle;
    s3c_fb_info-&gt;screen_base    = fb_va;

    /* 把framebuffer的地址告诉LCD控制器 */
    s3c_lcd_regs-&gt;lcdsaddr1 = (s3c_fb_info-&gt;fix.smem_start &gt;&gt; 1);
    s3c_lcd_regs-&gt;lcdsaddr2 = ((s3c_fb_info-&gt;fix.smem_start+320*240*4) &gt;&gt; 1) &amp; 0x1fffff;
    s3c_lcd_regs-&gt;lcdsaddr3 = 320*4/2;

    /* 使能LCD */
    s3c_lcd_regs-&gt;lcdcon1 |= (1&lt;&lt;0);

    /* 4. register_framebuffer */
    printk("%s %d\n", __FUNCTION__, __LINE__);
    //debug_lcd = 1;
    register_framebuffer(s3c_fb_info);
    printk("%s %d\n", __FUNCTION__, __LINE__);

    return 0; 
}

void s3c_lcd_exit(void)
{
    unregister_framebuffer(s3c_fb_info);
    dma_free_writecombine(NULL, s3c_fb_info-&gt;fix.smem_len, fb_va, s3c_fb_handle);
    iounmap(s3c_lcd_regs);
    iounmap(gpccon);
    iounmap(gpdcon);
    iounmap(gpgcon);
    framebuffer_release(s3c_fb_info);
}

module_init(s3c_lcd_init);
module_exit(s3c_lcd_exit);

MODULE_LICENSE("GPL");
</code></pre>

<p>然后把它加入到内核,以静态加载的模式启动.</p>

<p>最后,可以把读取内存jpeg格式数据输出到LCD屏的这部分整合到mjpg-stream或servfox去,就实现了采集图像本地显示了.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[servfox分析]]></title>
    <link href="http://hmgle.github.com/blog/2011/09/11/servfoxe58886e69e90/"/>
    <updated>2011-09-11T13:57:20+08:00</updated>
    <id>http://hmgle.github.com/blog/2011/09/11/servfoxe58886e69e90</id>
    <content type="html"><![CDATA[<h1><a href="http://hmgle.com/wiki/servfox_ans.html">servfox分析</a></h1>

<p>--by:hmgle Copyleft: <a href="http://creativecommons.org/licenses/by-nc-sa/3.0/deed.zh">CC BY-NC-SA</a></p>

<p>构建嵌入式Linux网络视频监控系统中,我们采用servfox来做服务器采集程序. servfox涉及到的内容主要有:V4L1接口、套接字和多线程编程. 这里简单分析一下servfox-R1_1_3.</p>

<h2>1. servfox做了什么?</h2>

<p>servfox在采集图像的过程中主要做什么事情?
它初始化摄像头设备后创建了线程1采集视频图像. 然后主程序创建一个套接字监听,阻塞等待客户端的请求连接. 连接成功后再创建线程2发送采集到的图像数据给客户端.</p>

<ul>
<li><p>线程1:采集视频图像.</p></li>
<li><p>线程2:发送图像数据给客户端.</p></li>
</ul>


<p>在采集线程和发送线程同时运行的情况下,会存在对存储压缩过的图像数据的缓冲区这个临界区竞争的情况. 为了能把采集到的一帧图像数据完整地发送出去,需要采用一些同步机制.
servfox只是个应用程序,它初始化设备,获取设备属性和图像属性,设置图像参数,捕捉图像数据,都是通过Video4Linux接口标准调用驱动的相关函数完成的. 本文末尾将会列举部分摄像头设备驱动要实现的file_oerations结构体里面的函数.</p>

<h2>2. servfox运行步骤</h2>

<p>servfox运行流程图如下:</p>

<p><img src="http://hmgle.com/photo_html/servfox_ans_.png" alt="" /></p>

<h3>2.1 从命令行传递参数给变量</h3>

<p>main()函数内,首先执行的是一个for循环体. 看一下里面的几个语句:</p>

<pre><code>...
if (strcmp (argv[i], "-d") == 0) {
    if (i + 1 &gt;= argc) {
        if(debug)
            printf ("No parameter specified with -d, aborting.\n");
        exit (1);
    }
    videodevice = strdup (argv[i + 1]);
}
...
</code></pre>

<p>videodevice保存了摄像头设备节点名称. 用户不指定的话,后面会将它设置为"/dev/video0".</p>

<pre><code>...
if (strcmp (argv[i], "-g") == 0) {
    /* Ask for read instead default  mmap */
    grabmethod = 0;
}
...
</code></pre>

<p>通过grabmethod的设置就指定了采集图像时使用mmap()内存映射的方法还是read()读取的方法. 采用read系统调用来读取图像数据的话在连续抓取的情况下会发生频繁的用户态和内核态的切换,效率低. 通过mmap内存映射的话,把摄像头对应的设备文件映射到进程内存中,减少I/O操作,提高了效率. 因此启动servfox时不加"-g"选项的话默认采用grabmethod=1为mmap方式.</p>

<p>在for循环体里面还根据用户输入的选项分配了存储分辨率大小width/height,创建套接字时用的端口号serverport(默认为7070).</p>

<h3>2.2 初始化视频采集设备</h3>

<p>接下来主要要执行的语句有:</p>

<pre><code>memset (&amp;videoIn, 0, sizeof (struct vdIn)); // 将结构体videoIn初始化为0
</code></pre>

<p>先来看看videoIn这个结构体:</p>

<ul>
<li><p>vdIn 结构体(在spcav4l.h中定义,它里面的成员都是依据Video4Linux接口标准而定义的):</p>

<p>struct vdIn {</p>

<pre><code>int fd;             // 设备文件描述符
char *videodevice ;     // 设备,视频捕捉接口文件
struct video_mmap vmmap;
/* 用于内存映射方法时进行图像数据的获取,
 * 里面的成员.frame表示当前将获取的帧号,
 * 成员.height和.width表示图像高度和宽度,
 * 成员.format表示图像格式.
 */

struct video_capability videocap;
/* 包含设备的基本信息(设备名称,支持的最大最小分辨率,信号源信息等)
 */

int mmapsize;
struct video_mbuf videombuf;
/* 利用mmap映射到摄像头存储缓冲区的帧信息,
 * 包括帧的大小(size),最多支持的帧数(frames),
 * 每帧相对基址的偏移(offset)
 */

struct video_picture videopict;     // 采集到的图像的各种属性
struct video_window videowin;       // 包含capture area的信息
struct video_channel videochan;     // 各个信号源的属性
struct video_param videoparam;
int cameratype ;        // 是否能capture,彩色还是黑白,是否能裁剪等等
char *cameraname;       // 设备名称
char bridge[9];
int sizenative;         // available size in jpeg.
int sizeothers;         // others palette.
int palette;            // available palette.
int norme ;             // set spca506 usb video grabber.
int channel ;       // set spca506 usb video grabber 信号源个数
int grabMethod ;
unsigned char *pFramebuffer;    // 指向内存映射的指针
unsigned char *ptframe[4];      // 指向压缩后的帧的指针数组
int framelock[4];
pthread_mutex_t grabmutex;      // 视频采集线程和传输线程的互斥信号
int framesizeIn ;               // 视频帧的大小
volatile int frame_cour;        // 指向压缩后的帧的指针数组下标
int bppIn;      // 采集的视频帧的BPP
int hdrwidth;           // 采集的视频帧的宽度
int hdrheight;          // 采集的视频帧的高度
int formatIn;           // 采集的视频帧的格式
int signalquit;     // 停止视频采集的信号
</code></pre>

<p>};</p></li>
</ul>


<p>接下来执行:</p>

<pre><code>if (init_videoIn
            (&amp;videoIn, videodevice, width, height, format,grabmethod) != 0)
</code></pre>

<p>这个函数主要是设置了grabmethod:用mmap方式还是read方式;
设置videodevice成员设备文件名称,默认是 "/dev/video0";
设置信号vd->signalquit=1,图像宽高:vd->hdrwidth=width;vd->hdrheight=height;
设置图像格式为VIDEO_PALETTE_JPEG:vd->formatIn = format;
获得色深:vd->bppIn = GetDepth (vd->formatIn);</p>

<hr />

<p>调用init_v4l():
=================进入init_v4l()================================================
init_v4l()是初始化v4l视频设备的函数,它首先通过系统调用open()打开视频设备,成功打开后主要执行下面几个步骤:</p>

<ul>
<li><ol>
<li>通过系统调用ioctl (vd->fd, VIDIOCGCAP, &amp;(vd->videocap))取得设备信息。读取struct video_capability中有关摄像头的信息,保存到vd->videocap中.</li>
</ol>
</li>
<li><ol>
<li>初始化图像.</li>
</ol>
</li>
</ul>


<p>ioctl (vd->fd, VIDIOCGPICT, &amp;vd->videopict);
带VIDIOCGPICT参数的ioctl调用会获取图像的属性,并保存在vd->videopict指向的结构体中.</p>

<ul>
<li><ol>
<li>读取ruct video chanel中有关设备通道的信息，保存到vd->videochan指向的结构体中。</li>
</ol>
</li>
</ul>


<p>ioctl (vd->fd, VIDIOCGCHAN, &amp;vd->videochan);</p>

<ul>
<li><ol>
<li>设置摄像头参数.</li>
</ol>
</li>
</ul>


<p>读取摄像头数据前,需要对摄像头进行设置,主要包括图像参数和分辨率.</p>

<pre><code>ioctl (vd-&gt;fd, VIDIOCSPICT, &amp;vd-&gt;videopict)
</code></pre>

<p>设置分辨率主要是对vd->videowin各分量进行修改,若为read方式,具体实现为:</p>

<pre><code>if (ioctl (vd-&gt;fd, VIDIOCGWIN, &amp;(vd-&gt;videowin)) &lt; 0)    // 获得捕获源的大小
    perror ("VIDIOCGWIN failed \n");
vd-&gt;videowin.height = vd-&gt;hdrheight;
vd-&gt;videowin.width = vd-&gt;hdrwidth;
if (ioctl (vd-&gt;fd, VIDIOCSWIN, &amp;(vd-&gt;videowin)) &lt; 0)
    perror ("VIDIOCSWIN failed \n");
</code></pre>

<ul>
<li><ol>
<li>摄像头设备文件映射初始化或read方式初始化</li>
</ol>
</li>
</ul>


<p>完成上述初始化设备工作后,就可以对访问到摄像头设备文件的内容了. 如果选用mmap()内存映射方式的话,下面的步骤将摄像头设备文件映射到进程内存,这样就可以直接读取映射了的这片内存,而不必read设备文件了:</p>

<p>a. 获取摄像头缓冲区帧信息:</p>

<pre><code>ioctl (vd-&gt;fd, VIDIOCGMBUF, &amp;(vd-&gt;videombuf));
</code></pre>

<p>该操作获取摄像头存储缓冲区的帧信息:包括帧的大小(size),最多支持的帧数(frames),每帧相对基址的偏移(offset). 这些参数都是由摄像头设备硬件决定的. 这些信息将被保存在videombuf结构体里面,下面的映射摄像头设备文件到内存操作马上就要用到了:</p>

<p>b. 映射摄像头设备文件到内存:</p>

<pre><code>vd-&gt;pFramebuffer =
            (unsigned char *) mmap (0, vd-&gt;videombuf.size, PROT_READ | PROT_WRITE,
                    MAP_SHARED, vd-&gt;fd, 0);
</code></pre>

<p>该操作把摄像头对应的设备文件映射到内存区. 该映射内容区可读可写并且不同进程间可共享. 帧的大小(vd->videombuf. size)是a步骤获取的. 该函数成功返回映像内存区的指针,该指针赋值给vd->pFramebuffer,失败时返回-1.</p>

<p>c. 视频图像捕捉测试:</p>

<pre><code>/* Grab frames 抓取一帧*/
if (ioctl(vd-&gt;fd, VIDIOCMCAPTURE, &amp;(vd-&gt;vmmap))) {
    perror ("cmcapture");
}
</code></pre>

<p>该操作捕捉一帧图像,获取图像信息到vmmap里. 它会根据vmmap中设置的属性参数(frame,height,width和format)通知驱动程序启动摄像头抓拍图像. 该操作是非阻塞的,是否截取完毕留给VDIOCSYNC来判断. 在init_v4l()这里只是为了测试是否可以成功捕获一帧图像,真正采集图像是在采集线程时执行v4lGrab()这个函数的时候.</p>

<p>以上是用mmap内存映射方式,如果采用直接读取摄像头设备文件的方式获取图像的话,将执行:</p>

<pre><code>els {
    /* read method */
    /* allocate the read buffer */
    vd-&gt;pFramebuffer = (unsigned char *) realloc(vd-&gt;pFramebuffer, \
            (size_t) vd-&gt;framesizeIn);
    /* 为pFrameffer分配内存 */

    if (ioctl (vd-&gt;fd, VIDIOCGWIN, &amp;(vd-&gt;videowin)) &lt; 0)    // 获得捕获源的大小
        perror("VIDIOCGWIN failed \n");
    vd-&gt;videowin.height = vd-&gt;hdrheight;
    vd-&gt;videowin.width = vd-&gt;hdrwidth;
    if (ioctl(vd-&gt;fd, VIDIOCSWIN, &amp;(vd-&gt;videowin)) &lt; 0)
        perror("VIDIOCSWIN failed \n");
}
</code></pre>

<p>摄像头设备文件映射初始化或read方式初始化完成后,返回init_videoIn().</p>

<p>=============从init_v4l() 返回========================================</p>

<hr />

<p>从init_v4l()返回到init_videoIn()后,分配vd->ptframe[i]空间.</p>

<pre><code>for (i = 0; i &lt; OUTFRMNUMB; i++) {
    vd-&gt;ptframe[i] = NULL;
    vd-&gt;ptframe[i] = (unsigned char *) realloc (vd-&gt;ptframe[i],\
            sizeof(struct frame_t) + (size_t) vd-&gt;framesizeIn );
    vd-&gt;framelock[i] = 0;
}
</code></pre>

<p>unsigned char* ptframe[4]：指向四个buffer缓冲数组，用来存放已压缩完成的图像数据.</p>

<h3>2.3 采集图像数据线程</h3>

<p>init_videoIn()执行完后返回main(),接下来创建采集视频图像的线程:</p>

<pre><code>pthread_create (&amp;w1, NULL, (void *) grab, NULL);
</code></pre>

<p>进入grab()函数:可以看到在死循环体里面调用v4lGrab()函数.
进入v4lGrab()函数,先判断一下是用mmap方法还是用read方法. 下面仅就mmap方法分析:</p>

<pre><code>ioctl (vd-&gt;fd, VIDIOCSYNC, &amp;vd-&gt;vmmap.frame);
</code></pre>

<p>这条语句是等待捕捉完这一帧图像,调用成功后表明一帧图像捕捉完毕,可以开始进行下一次图像捕捉. vd->vmmap.frame是当前捕捉到帧的序号.</p>

<p>接下来的是个循环睡眠等待:</p>

<pre><code>while((vd-&gt;framelock[vd-&gt;frame_cour] != 0) &amp;&amp; vd-&gt;signalquit)
            usleep(1000);
</code></pre>

<p>它是等待之后执行的另一个用来的发送采集到的图像数据给客户端的线程,直到它把这一帧图像完整地发送出去. 每隔1毫秒就检查一次是否发完. 如果不等待就执行下面的操作的话,那么还没发送完就把本来要发送的图像数据重写掉,采集到的数据没用上. 可以采用更好的同步机制--信号量来实现.</p>

<p>等到上一帧图像数据发送出去之后,这个线程等待直到获得一把线程互斥锁:</p>

<pre><code>pthread_mutex_lock (&amp;vd-&gt;grabmutex);
</code></pre>

<p>它把临界区资源vd->ptframe锁住,防止下面获取时间和拷贝数据到ptframe及设置一帧图像的头部时被别的线程抢占. 虽然在发送线程里并没有找到相关互斥锁的操作(这个应该是要加的),但为了扩展,有可能以后我们添加一些访问临界区vd->ptframe的线程时可以用它这把锁.</p>

<p>然后执行:</p>

<pre><code>  tems = ms_time();
</code></pre>

<p>tems获得的是距离UNIX的Epoch时间即:1970年1月1日0时0分0秒算起的毫秒数. 它可以用在视频图像的时间戳.
然后执行:</p>

<pre><code>jpegsize= convertframe(vd-&gt;ptframe[vd-&gt;frame_cour]+ sizeof(struct frame_t),
                vd-&gt;pFramebuffer + vd-&gt;videombuf.offsets[vd-&gt;vmmap.frame],
                vd-&gt;hdrwidth,vd-&gt;hdrheight,vd-&gt;formatIn,vd-&gt;framesizeIn);
</code></pre>

<p>跟踪进去可以看出要是视频图像格式是VIDEO_PALETTE_JPEG的话,直接将pFramebuffer中的数据拷贝到ptframe缓存中去，而不压缩处理,因为获得的就是已经压缩过的jpeg格式了(是硬件或底层驱动做了,一般USB摄像头对采集到的图像都作了jpeg格式压缩(内置JPEG硬件压缩)). 获得jpeg格式文件的大小是通过调用get_jpegsize()实现的. 进入get_jpegsize()可以发现,它利用了jpeg文件格式中是以0xFF 0xD9结尾的这个特性. ptframe里面的经压缩过的图像数据就是发送线程要发送出去的内容了.</p>

<p>pFramebuffer中的数据拷贝进ptframe完成后,就截取下一帧图像数据了:</p>

<pre><code>/* Grab frames */
if ((ioctl (vd-&gt;fd, VIDIOCMCAPTURE, &amp;(vd-&gt;vmmap))) &lt; 0) {
    perror ("cmcapture");
    if(debug) printf ("&gt;&gt;cmcapture err \n");
    erreur = -1;
}
    vd-&gt;vmmap.frame = (vd-&gt;vmmap.frame + 1) % vd-&gt;videombuf.frames;
    vd-&gt;frame_cour = (vd-&gt;frame_cour +1) % OUTFRMNUMB;
</code></pre>

<p>执行完后,跳出v4lGrab()函数体,返回到grab()去. 正常运行状态下,将不断循环调用v4lGrab()采集图像数据. 采集线程分析完毕.</p>

<h3>2.4 建立TCP套接字服务端,为图像数据发送线程做好准备</h3>

<p>回到main(),继续往下执行:</p>

<pre><code>serv_sock = open_sock(serverport);
</code></pre>

<p>跟踪进入open_sock()里面可以看到通过执行socket(),bind(),listen()建立了一个TCP套接字服务端并在指定端口上监听,等待客户端连接. 紧跟在socket()后面有一句:</p>

<pre><code>setsockopt(server_handle, SOL_SOCKET, SO_REUSEADDR, &amp;O_on, sizeof (int));
</code></pre>

<p>这个语句应该是为了允许启动多个服务端或多个servfox. 参见:<a href="http://blog.csdn.net/liusujian02/article/details/1944520">http://blog.csdn.net/liusujian02/article/details/1944520</a> (关于SO_REUSEADDR的使用说明)</p>

<p>执行完serv_sock = open_sock(serverport)这个语句之后,下一条语句是:</p>

<pre><code>signal(SIGPIPE, SIG_IGN);   /* Ignore sigpipe */
</code></pre>

<p>这是为了忽略SIGPIPE信号:若客户端关闭了和服务端的连接,但服务端依然试图发送图像数据给客户端(write to pipe with no readers),系统就会发出一个SIGPIPE信号,默认对SIGPIPE的处理是terminate(终止),那么负责发送图像数据的服务端就挂掉了,即使还有别的客户端连接. 这当然不是我们想要的,因此把我们要执行这句语句把SIGPIPE信号忽略掉.</p>

<h3>2.5 发送图像数据到客户端的线程</h3>

<p>接下来,是一个while(videoIn.signalquit)循环体,如果没有接收到退出信号,它就一直循环运行里面的语句:</p>

<pre><code>while (videoIn.signalquit) {
    sin_size = sizeof(struct sockaddr_in);

    /* 等待客户端的连接，如果没有连接就一直阻塞下去，
     * 如果有客户连接就创建一个线程，
     * 在新的套接口上与客户端进行数据交互
     */
    if ((new_sock = accept(serv_sock, (struct sockaddr *)&amp;their_addr, &amp;sin_size)) == -1) {
            continue;
    }
    syslog(LOG_ERR,"Got connection from %s\n",inet_ntoa(their_addr.sin_addr));
    printf("Got connection from %s\n",inet_ntoa(their_addr.sin_addr));
    pthread_create(&amp;server_th, NULL, (void *)service, &amp;new_sock);
}
</code></pre>

<p>之前建立的服务端一直监听等待客户端来连接,一旦有客户端connect()过来,服务端执行accept()建立连接后,就创建了发送图像数据到客户端的线程了:</p>

<pre><code>pthread_create(&amp;server_th, NULL, (void *)service, &amp;new_sock);
</code></pre>

<p>我们再进入这个线程执行的service()函数去分析:</p>

<hr />

<p>=============进入service()==============================</p>

<pre><code>/* initialize video setting */
    bright = upbright(&amp;videoIn);
    contrast = upcontrast(&amp;videoIn);
    bright = downbright(&amp;videoIn);
    contrast = downcontrast(&amp;videoIn);
</code></pre>

<p>上面所谓的初始话视频设置,是先增大一下亮度和对比度,在减小亮度和对比度恢复到原来的状态,顺便将亮度值保存在bright变量,将对比度值保存在contrast变量.
然后是一个死循环体:</p>

<pre><code>for ( ; ; ) {
    memset(&amp;message,0,sizeof(struct client_t));
    ret = read(sock,(unsigned char*)&amp;message,sizeof(struct client_t));
    ......
    if (message.updobright){
        switch (message.updobright){
            case 1: bright = upbright(&amp;videoIn);
                break;
            case 2: bright = downbright(&amp;videoIn);
                break;
        }
        ack = 1;
    } else if (message.updocontrast){
        switch (message.updocontrast){
            case 1: contrast = upcontrast(&amp;videoIn);
                break;
            case 2: contrast = downcontrast(&amp;videoIn);
                break;
        }
        ack = 1;
    } else if (message.updoexposure){
        switch (message.updoexposure){
            case 1: spcaSetAutoExpo(&amp;videoIn);
                break;
            case 2:;
                break;
        }
        ack = 1;
    } else if (message.updosize){ //compatibility FIX chg quality factor ATM
        switch (message.updosize){
            case 1: qualityUp(&amp;videoIn);
                break;
            case 2: qualityDown(&amp;videoIn);
                break;
        }
        ack = 1;
    } else if (message.fps){
        switch (message.fps){
            case 1: timeDown(&amp;videoIn);
                break;
            case 2: timeUp(&amp;videoIn);
                break;
        }
        ack = 1;
    } else if (message.sleepon){
        ack = 1;
    } else ack =0;
    while ((frameout == videoIn.frame_cour) &amp;&amp; videoIn.signalquit)
        usleep(1000);
    if (videoIn.signalquit){
        videoIn.framelock[frameout]++;
        headerframe = (struct frame_t *) videoIn.ptframe[frameout];
        headerframe-&gt;acknowledge = ack;
        headerframe-&gt;bright = bright;
        headerframe-&gt;contrast = contrast;
        headerframe-&gt;wakeup = wakeup;
        ret = write_sock(sock, (unsigned char *)headerframe, sizeof(struct frame_t)) ;
        /* 发送帧信息头 */

        if(!wakeup)
            ret = write_sock(sock,(unsigned char*)(videoIn.ptframe[frameout] + \
                        sizeof(struct frame_t)),headerframe-&gt;size);

        videoIn.framelock[frameout]--;
        frameout = (frameout+1)%4;
    } else {
        if(debug)
            printf("reader %d going out \n",*id);
        break;
    }
}
</code></pre>

<p>和客户端建立连接后,客户端会先将设置图像的信息发给服务端,因此上面代码,首先读取客户端对图像的设置,把设置信息存放在message结构体里,然后是根据message里的信息对采集图像的显示属性(如亮度bright,对比度contrast等)进行设置,具体操作是通过ioctl()调用底层驱动来完成对摄像头抓拍图像的显示设置.</p>

<p>设置完采集图像显示属性后,执行:</p>

<pre><code>while ((frameout == videoIn.frame_cour) &amp;&amp; videoIn.signalquit)
    usleep(1000);
</code></pre>

<p>frame_cour是指向压缩后的图像帧的指针数组下标,我们一共存储4帧(unsigned char *ptframe[4]),为了按顺序读取每一帧,就等待知道frameout和videoIn.frame_cour相等时才执行后面的发送操作,发送这帧图像完成后会执行frameout = (frameout+1)%4使得下一次发送下一帧图像. 个人觉得这里采取信号量的同步机制更好.
等采集线程完成一帧采集使得videoIn.frame_cour等于frameout之后(因为这里没有采用同步机制,有可能这一轮会落空),就开始执行发送这一帧图像给客户端的操作了:先将让headerframe指向帧信息头,然后发送headerframe指向的信息头给客户端,再发送剩下的图像数据. 这样就把完整的一帧图像发送给客户端了.
只要没有收到客户端退出的信号,以上的发送过程会循环执行.</p>

<p>当收到客户端退出的信息后,它就退出循环,执行close_sock(sock)关闭套接字,终止线程.</p>

<p>=============从service()返回=================================</p>

<hr />

<p>服务器发送图像线程终止后,只要进程没有退出信号还会在while (videoIn.signalquit)这个循环体继续,阻塞等待客户端的连接,重复上面的过程.</p>

<p>若videoIn.signalquit等于0了,就不再执行这个循环体,等待采集线程退出:pthread_join (w1, NULL);关闭套接字:close(serv_sock);回收以前分配的资源:close_v4l (&amp;videoIn);整个程序就正常退出了.</p>

<h2>3. servfox与底层驱动的接口</h2>

<p>前面说过,servfox只是个应用程序,它初始化设备,获取设备属性和图像属性,设置图像参数,捕捉图像数据,都是通过V4L1接口标准调用驱动的相关函数完成的.V4L1就是Video4Linux的版本1,Video4Linux已整合进Linux内核里面了.新版本是v4l2,它和v4l1不是完全兼容的.而V4L1已经是过时了.从Linux 2.6.38 内核就已经完全放弃了对v4l1的支持,因此不修改过的servfox不能在2.6.38以上的内核上运行了.不过有功能更强大的mjpeg-streamer来取代servfox.而mjpeg-streamer是基于v4l2接口的.</p>

<p>由于servfox体积小,在它上面进行扩展是很容易的,比如加入基于libjpeg库的本地解码jpeg显示到lcd屏的线程,加入截屏的线程等.</p>

<p>下面列出了servfox用到的一些v4l1的接口,如果非要把servfox移植到2.6.38的Linux内核上运行的话,必须修改这些v4l1的接口使之兼容于v4l2.</p>

<ul>
<li><p>摄像头驱动里要实现的ioctl()</p>

<ol>
<li><p>ioctl(vd->fd, VIDIOCSYNC, &amp;vd->vmmap.frame)</p>

<p> /<em> VIDIOCSYNC: Sync with mmap grabbing </em>/</p>

<p> /* 等待捕捉到这一帧图象.</p>

<ul>
<li>即等待一帧截取结束.</li>
<li>若成功，表明一帧截取已完成。</li>
<li>可以开始做下一次 VIDIOCMCAPTURE
*/</li>
</ul>
</li>
<li><p>if ((ioctl (vd->fd, VIDIOCMCAPTURE, &amp;(vd->vmmap))) &lt; 0)</p>

<p> /* Mmap方式下做视频截取的 VIDIOCMCAPTURE.</p>

<ul>
<li>若调用成功，开始一帧的截取，是非阻塞的，</li>
<li>是否截取完毕留给VIDIOCSYNC来判断
*/</li>
</ul>
</li>
<li><p>读video_picture中信息
 ioctl(vd->fd, VIDIOCGPICT, &amp;(vd->picture))；</p></li>
</ol>


<p>if (ioctl (vd->fd, VIDIOCGPICT, &amp;vd->videopict) &lt; 0)    /<em> Get picture properties </em>/
访问摄像头设备采集的图像的各种属性。然后通过访问结构体vd->videopict 就可以读出图像的各种信息。
vd->videopict中分量的值是可以改变的，实现方法为：先为分量赋新值，再调用VIDIOCSPICT. 如:</p>

<ol>
<li><p>  if (ioctl (vd->fd, VIDIOCGCAP, &amp;(vd->videocap)) == -1)      /<em> Get capabilities </em>/</p>

<pre><code>  exit_fatal ("Couldn't get videodevice capability");
</code></pre>

<p>  读video_capability 中信息
  ioctl(vd->fd, VIDIOCGCAP, &amp;(vd->capability))
  成功后可读取vd->capability各分量  eg.
  Printf（”maxwidth = %d”vd->capability.maxwidth）;</p></li>
<li><p>初始化channel:
if (ioctl (vd->fd, VIDIOCGCHAN, &amp;vd->videochan) == -1)     /<em> Get channel info (sources) </em>/</p>

<pre><code> // 用来取得和设置channel信息，例如使用那个输入源，制式等
</code></pre>

 {

<pre><code> if(debug) printf ("Hmm did not support Video_channel\n");
 vd-&gt;cameratype = UNOW;
</code></pre>

<p> }</p></li>
<li><p>初始化video_mbuf，以得到所映射的buffer的信息
 ioctl(vd->fd, VIDIOCGMBUF, &amp;(vd->mbuf))
 if (ioctl (vd->fd, VIDIOCGMBUF, &amp;(vd->videombuf)) &lt; 0)      /<em> Memory map buffer info </em>/
 {</p>

<pre><code> perror (" init VIDIOCGMBUF FAILED\n");
</code></pre>

<p> }</p>

<p> // 要确定是否捕捉到图象，要用到下一个命令。</p>

<pre><code>     if (ioctl (vd-&gt;fd, VIDIOCMCAPTURE, &amp;(vd-&gt;vmmap)))   /* Grab frames 抓取帧*/
     {
         perror ("cmcapture");
     }
</code></pre></li>
<li><p>if (ioctl (vd->fd, VIDIOCGWIN, &amp;(vd->videowin)) &lt; 0) // 获得捕获源的大小</p>

<pre><code>     perror ("VIDIOCGWIN failed \n");
</code></pre></li>
</ol>
</li>
<li><p>v4l2中的ioctl()的cmd:</p></li>
</ul>


<p>在进行V4L2开发中，一般会用到以下的命令标志符：</p>

<ol>
<li><p>VIDIOC_REQBUFS：分配内存</p></li>
<li><p>VIDIOC_QUERYBUF：把VIDIOC_REQBUFS中分配的数据缓存转换成物理地址</p></li>
<li><p>VIDIOC_QUERYCAP：查询驱动功能</p></li>
<li><p>VIDIOC_ENUM_FMT：获取当前驱动支持的视频格式</p></li>
<li><p>VIDIOC_S_FMT：设置当前驱动的频捕获格式</p></li>
<li><p>VIDIOC_G_FMT：读取当前驱动的频捕获格式</p></li>
<li><p>VIDIOC_TRY_FMT：验证当前驱动的显示格式</p></li>
<li><p>VIDIOC_CROPCAP：查询驱动的修剪能力</p></li>
<li><p>VIDIOC_S_CROP：设置视频信号的边框</p></li>
<li><p>VIDIOC_G_CROP：读取视频信号的边框</p></li>
<li><p>VIDIOC_QBUF：把数据从缓存中读取出来</p></li>
<li><p>VIDIOC_DQBUF：把数据放回缓存队列</p></li>
<li><p>VIDIOC_STREAMON：开始视频显示函数</p></li>
<li><p>VIDIOC_STREAMOFF：结束视频显示函数</p></li>
<li><p>VIDIOC_QUERYSTD：检查当前视频设备支持的标准，例如PAL或NTSC。</p></li>
</ol>


<p>这些IO调用，有些是必须的，有些是可选择的。</p>

<p>=========END=====================================================</p>

<ul>
<li><p>参考文献</p></li>
<li><p>基于S3C2440的嵌入式视频网络监控系统--柳亚东</p></li>
<li>基于嵌入式ARM的远程视频监控系统研究--李保国</li>
<li>基于ARM的嵌入式网络视频监控系统设计与实现--方卫民
......</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[读阿城小说是一种享受]]></title>
    <link href="http://hmgle.github.com/blog/2011/06/18/e8afbbe998bfe59f8ee5b08fe8afb4e698afe4b880e7a78de4baabe58f97/"/>
    <updated>2011-06-18T13:57:41+08:00</updated>
    <id>http://hmgle.github.com/blog/2011/06/18/e8afbbe998bfe59f8ee5b08fe8afb4e698afe4b880e7a78de4baabe58f97</id>
    <content type="html"><![CDATA[<p>有时间的话，想读读阿城的小说。以前只读过他的《棋王》——多年前的事情了。
附上豆瓣上的一篇评论链接：<a href="http://book.douban.com/review/1421693/">http://book.douban.com/review/1421693/</a>
当然先要把那本《<a href="http://book.douban.com/subject/2175630/">8051微控制器和嵌入式系统</a>》看完。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[删除线]]></title>
    <link href="http://hmgle.github.com/blog/2008/06/12/e588a0e999a4e7babf/"/>
    <updated>2008-06-12T23:07:00+08:00</updated>
    <id>http://hmgle.github.com/blog/2008/06/12/e588a0e999a4e7babf</id>
    <content type="html"><![CDATA[<p>方法一：可以在文字处理软件里编辑然后复制进来,这段文字就是这样的。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[回来了]]></title>
    <link href="http://hmgle.github.com/blog/2008/06/03/e59b9ee69da5e4ba86/"/>
    <updated>2008-06-03T08:16:00+08:00</updated>
    <id>http://hmgle.github.com/blog/2008/06/03/e59b9ee69da5e4ba86</id>
    <content type="html"><![CDATA[<p>Ping request could not find host www.google.com. Please check the name and try again.。</p>

<p>我神经很混乱了。</p>

<p>???????</p>
]]></content>
  </entry>
  
</feed>
